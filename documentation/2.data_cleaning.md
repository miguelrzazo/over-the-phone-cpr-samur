

# Documentación del Proceso de Limpieza de Datos
# RCP Transtelefónica - Proyecto de Investigación


## 1. Visión General del Proceso de Limpieza

El proceso de limpieza de datos de este proyecto es un pilar fundamental para garantizar la validez y reproducibilidad de los análisis estadísticos sobre el impacto de la RCP transtelefónica. El script principal (`cleaning.py`) implementa de forma programática y transparente todas las reglas clínicas y metodológicas definidas en `Reglas_exclusion.md`, permitiendo transformar datos crudos heterogéneos en un dataset robusto, homogéneo y listo para análisis avanzado.

**¿Qué hemos hecho y por qué?**

1. **Estandarización y control de calidad**: Hemos uniformizado todos los formatos de variables, eliminando inconsistencias y asegurando que los datos sean comparables entre sí. Esto incluye la conversión de todas las variables numéricas a números naturales (enteros, sin decimales), lo que elimina posibles errores de interpretación y facilita la integración con análisis estadísticos y tablas para publicación.

2. **Aplicación rigurosa de criterios de exclusión**: Se han implementado reglas clínicas estrictas para excluir casos no relevantes (por ejemplo, paradas de origen traumático o sin información de RCP transtelefónica), utilizando búsquedas robustas de palabras clave (con y sin tilde, sinónimos) en los campos de texto. Esto asegura que la cohorte final sea representativa y adecuada para responder a las preguntas de investigación.

3. **Fusión inteligente de registros**: Para evitar duplicidades y maximizar la sensibilidad en la identificación de casos de RCP transtelefónica, se fusionan registros de unidades SVB y SVA por fecha/hora, priorizando la información más relevante según reglas clínicas. Así, ningún caso positivo de RCP transtelefónica se pierde por cuestiones administrativas o de registro.

4. **Generación de variables derivadas y codificación clínica**: Se han calculado variables clave como el tiempo de llegada, el tipo de respondiente, el ritmo inicial, ROSC, supervivencia y CPC, aplicando lógica clínica y reglas de negocio específicas del estudio. Esto permite que el dataset final contenga toda la información necesaria para los análisis descriptivos e inferenciales requeridos.

5. **Gestión transparente de datos faltantes**: El script deja explícitamente vacíos los campos principales cuando no hay información, salvo en variables booleanas, que se rellenan a 0 para evitar falsos positivos. Esta decisión metodológica prioriza la transparencia y evita sobreestimar resultados positivos por imputaciones automáticas.

6. **Automatización del control de calidad manual**: Se genera automáticamente un informe de anomalías en Markdown (`informe_anomalias.md`) que identifica casos críticos para revisión clínica: supervivientes sin CPC y filas con 4 o más campos principales vacíos. Este informe es una herramienta clave para la trazabilidad y la validación final del dataset, permitiendo documentar decisiones clínicas o correcciones manuales de forma estructurada y reproducible.

7. **Reproducibilidad total**: Todo el pipeline es ejecutable con un solo comando, y cada paso es auditable y documentado. Esto permite que cualquier miembro del equipo (o revisor externo) pueda replicar exactamente el proceso y obtener los mismos resultados, cumpliendo los estándares más exigentes de ciencia abierta y reproducibilidad.

En resumen, el proceso de limpieza no solo transforma los datos, sino que añade valor científico al garantizar que los análisis posteriores se basan en una cohorte bien definida, sin sesgos evitables y con trazabilidad completa de cada decisión tomada durante la preparación de los datos.

### 1.1 Objetivos de la Limpieza de Datos

- **Uniformizar formatos**: Convertir variables a formatos consistentes
- **Eliminar inconsistencias**: Corregir valores incoherentes o imposibles
- **Aplicar criterios de exclusión**: Implementar criterios para excluir casos no relevantes
- **Generar variables derivadas**: Calcular variables necesarias para el análisis
- **Preservar información clave**: Mantener los casos con RCP transtelefónica

### 1.2 Verificación Manual y Reporte de Anomalías

El script genera automáticamente un informe en Markdown (`informe_anomalias.md`) que facilita la comprobación manual de los siguientes casos:
- Casos con supervivencia pero sin CPC asignado
- Casos con 4 o más campos principales vacíos

Este informe permite al equipo investigador revisar rápidamente las anomalías y documentar decisiones clínicas o correcciones manuales, garantizando la trazabilidad y calidad final del dataset.

## 2. Pipeline de Limpieza de Datos

### 2.1 Lectura y Preprocesamiento Inicial

1. **Lectura de datos brutos**: Los datos se importan del archivo CSV original (`rawdata_2year.csv`)
2. **Renombrado de columnas**: Se aplica un esquema estandarizado de nombres para todas las columnas relevantes
3. **Conversión de texto**: Todas las columnas de texto se convierten a minúsculas para búsquedas robustas de palabras clave (incluyendo variantes con y sin tilde)
4. **Unión SVB-SVA**: Se fusionan datos de unidades básicas y avanzadas por fecha/hora (no por número de informe). Si cualquiera de los registros fusionados tiene RCP transtelefónica=1, el resultado fusionado será 1. El resto de variables prioriza SVA, usando SVB solo si SVA está vacío.

### 2.2 Transformaciones Principales

1. **Columnas booleanas**: Conversión estricta a valores binarios (0/1) para `rcp_transtelefonica`, `desa_externo`, `rcp_testigos`, `rosc`, `supervivencia_7dias`.
2. **Variables numéricas**: Todas las variables numéricas (edad, tiempos, etc.) se convierten a enteros naturales. No se permiten floats en el dataset final.
3. **Variables temporales**: `tiempo_llegada` se calcula como la suma de los intervalos parciales (`C0_C1`, `C1_C2`, `C2_C3`). `tiempo_rcp` se extrae o calcula según reglas clínicas y textuales.
4. **Procesamiento de ritmos cardíacos**: Ritmos desfibrilables (FV, TV, etc.) se codifican como 1; el resto como 0, usando sinónimos y variantes.
5. **Clasificación de respondientes**: Se detecta el tipo de respondiente (lego, sanitario, policía, bombero) mediante palabras clave en los campos de texto, priorizando la lógica clínica y las reglas del estudio.
6. **Determinación de ROSC**: Se asigna 1 si hay mención a hospital, ROSC o recuperación en los textos; 0 si hay exitus/fallecimiento. Si ROSC=0, el tiempo de RCP se estima según patrones textuales o diferencias horarias.
7. **Determinación de supervivencia y CPC**: Supervivencia y CPC se asignan según reglas clínicas y textuales, priorizando la detección de palabras clave y la lógica conservadora (CPC=5 si fallecido o sin datos claros).

## 3. Reglas de Exclusión Aplicadas

### 3.1 Exclusiones y Reglas de Fusión

1. **Casos traumáticos**: Se excluyen casos donde la parada cardiorrespiratoria tiene origen traumático, detectados por palabras clave (sinónimos y variantes con/sin tilde) en los campos de texto principales.
2. **Casos sin RCP transtelefónica**: Se excluyen registros donde la variable RCP transtelefónica está vacía o no informada.
3. **Fusión SVB-SVA**: La fusión se realiza por fecha/hora. Si cualquiera de los registros fusionados tiene RCP transtelefónica=1, el resultado fusionado será 1. El resto de variables prioriza SVA, usando SVB solo si SVA está vacío. Los SVB no emparejados se eliminan.
4. **Tratamiento de datos faltantes**: Si una variable principal está vacía, se deja como tal (NaN o vacío), salvo en variables booleanas, que se rellenan a 0.

## 4. Reordenamiento y Estructura Final


### 4.1 Variables Finales (Orden de Columnas)

1. `n_informe`: Identificador único del caso
2. `fecha`: Fecha y hora del incidente
3. `edad`: Edad del paciente (entero, sin decimales)
4. `sexo`: Género del paciente
5. `rcp_transtelefonica`: ¿Se realizó RCP guiada por teléfono? (1=Sí, 0=No)
6. `tipo_respondiente`: Tipo de persona que realizó la RCP (lego, sanitario, policia, bombero)
7. `tiempo_llegada`: Tiempo de llegada de la unidad en segundos (entero)
8. `desa_externo`: ¿Se utilizó desfibrilador externo? (1=Sí, 0=No)
9. `ritmo_desfibrilable`: ¿El ritmo inicial era desfibrilable? (1=Sí, 0=No)
10. `tiempo_rcp`: Duración de la RCP en segundos (entero)
11. `rosc`: ¿Hubo recuperación de circulación espontánea? (1=Sí, 0=No)
12. `supervivencia_7dias`: ¿El paciente sobrevivió a los 7 días? (1=Sí, 0=No)
13. `cpc`: Criterio de perfusión cerebral (1-5, donde 1=sin secuelas, 5=muerte)


## 5. Limitaciones del Proceso de Limpieza

1. **Extracción de información textual**:
   - La identificación de outcomes y exclusiones depende de análisis de texto no estructurado, lo que puede dejar escapar casos atípicos o generar falsos positivos.
2. **Datos faltantes**:
   - El script deja vacíos los campos principales si no hay información, salvo en variables booleanas (rellenadas a 0). Esto puede subestimar algunos outcomes positivos.
3. **Fusión SVB-SVA**:
   - La fusión por fecha/hora puede dejar fuera algunos casos de SVB no emparejados, aunque se prioriza la sensibilidad para RCP transtelefónica.
4. **Ausencia de floats**:
   - Todas las variables numéricas se fuerzan a enteros, lo que puede redondear valores si existieran decimales en los datos originales.


## 6. Estadísticas de Exclusión y Resultados del Proceso

El script imprime en consola un resumen detallado de:
- Casos totales iniciales
- Casos excluidos por RCP transtelefónica desconocida
- Casos excluidos por origen traumático
- Casos SVB emparejados/no emparejados
- Campos con datos faltantes en el dataset final

Además, se generan estadísticas descriptivas de la cohorte final: distribución por tipo de RCP, respondiente, ritmo, ROSC, supervivencia y CPC, estratificadas por edad y tiempo de llegada.


## 7. Reproducibilidad y Garantía de Calidad

El pipeline de limpieza y procesamiento está completamente automatizado y documentado en el script `cleaning.py`, permitiendo la reproducción exacta de todos los pasos metodológicos y lógicos sobre cualquier dataset equivalente que cumpla la misma estructura y reglas.

**Importante:** Por motivos de protección de datos personales y confidencialidad clínica, los datos originales utilizados en este estudio no pueden ser compartidos públicamente ni distribuidos junto al código. Sin embargo, toda la lógica, reglas de exclusión, transformaciones y generación de informes son totalmente transparentes y auditables.

El script genera:
1. Dataset limpio en formato CSV (`data/3.cleaned_data/cleaned_data.csv`)
2. Dataset limpio en formato Excel (`data/3.cleaned_data/cleaned_data.xlsx`)
3. Estadísticas y resumen de exclusión en la consola
4. Informe de anomalías para comprobación manual (`data/2.Data_cleaning/informe_anomalias.md`)

La reproducibilidad metodológica está garantizada: cualquier equipo con acceso a datos equivalentes podrá replicar exactamente el pipeline, obtener los mismos outputs y auditar cada decisión tomada en el proceso. La combinación de procesamiento automatizado y revisión manual (facilitada por el informe de anomalías) proporciona la máxima garantía de calidad para los datos utilizados en los análisis estadísticos posteriores.